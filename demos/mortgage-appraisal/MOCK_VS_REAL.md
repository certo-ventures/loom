# üé≠ Mock vs üöÄ Real LLM Guide

## Quick Comparison

| Feature | Mock Mode üé≠ | Real Mode üöÄ |
|---------|-------------|-------------|
| **Cost** | FREE | ~$0.50-2.00 per run |
| **Speed** | 3-4 seconds | 30-60 seconds |
| **API Keys** | Not needed | Required |
| **Responses** | Hardcoded | Dynamic from LLMs |
| **PDF Parsing** | Supports real PDFs | Supports real PDFs |
| **Use Case** | Testing, demos | Production, real analysis |

## Mock Mode (Default) üé≠

### When to Use
- ‚úÖ Testing the workflow logic
- ‚úÖ Demonstrating the architecture
- ‚úÖ Development without costs
- ‚úÖ CI/CD pipelines
- ‚úÖ Learning the system

### How to Enable
```bash
# Environment variable
export USE_MOCK_LLM=true

# Or in .env file
echo "USE_MOCK_LLM=true" > demos/mortgage-appraisal/.env

# Run
npx tsx demos/mortgage-appraisal/run-demo.ts
```

### What Gets Mocked
- ‚úÖ Document extraction (returns hardcoded property data)
- ‚úÖ Criterion reviews (returns predefined evaluations)
- ‚ùå PDF parsing (REAL - if you provide a PDF file)
- ‚ùå Consolidation logic (REAL)
- ‚ùå Workflow orchestration (REAL)

## Real Mode üöÄ

### When to Use
- ‚úÖ Production appraisal reviews
- ‚úÖ Real data extraction
- ‚úÖ Actual quality assessment
- ‚úÖ Regulatory compliance
- ‚úÖ Client deliverables

### Setup Instructions

#### Step 1: Get API Keys

**Azure OpenAI** (Recommended for Enterprise):
1. Create Azure OpenAI resource in Azure Portal
2. Deploy models: `gpt-4`, `gpt-4-turbo`, `gpt-35-turbo`
3. Get: API Key, Endpoint, Deployment Names

**Anthropic Claude**:
1. Sign up at https://console.anthropic.com
2. Get API key from console
3. Models available: `claude-3-sonnet`, `claude-3-opus`

#### Step 2: Configure Environment

```bash
# Copy template
cp demos/mortgage-appraisal/.env.template demos/mortgage-appraisal/.env

# Edit .env file
nano demos/mortgage-appraisal/.env
```

Set these values:
```bash
USE_MOCK_LLM=false

# Azure OpenAI
AZURE_OPENAI_API_KEY=abc123...
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_GPT4_DEPLOYMENT=gpt-4
AZURE_OPENAI_GPT4_TURBO_DEPLOYMENT=gpt-4-turbo
AZURE_OPENAI_GPT35_DEPLOYMENT=gpt-35-turbo

# Anthropic
ANTHROPIC_API_KEY=sk-ant-...
```

#### Step 3: Run with Real LLMs

```bash
# Load environment
source demos/mortgage-appraisal/.env

# Run
npx tsx demos/mortgage-appraisal/run-demo.ts
```

Or inline:
```bash
USE_MOCK_LLM=false \
AZURE_OPENAI_API_KEY=your-key \
AZURE_OPENAI_ENDPOINT=https://your-endpoint \
npx tsx demos/mortgage-appraisal/run-demo.ts
```

### What Gets Called
- üöÄ **GPT-4**: Document extraction, detailed criterion reviews
- üöÄ **Claude-3**: Alternative perspective on criteria
- üöÄ **GPT-3.5**: Fast, cost-effective third opinion
- üöÄ **PDF Parser**: Real text extraction from PDFs
- ‚úÖ **Consolidation**: Real multi-agent reconciliation

### Cost Estimation

**Per Run** (10 criteria √ó 3 models):
- Document Extraction: ~1,500 tokens √ó $0.01/1K = $0.015
- 30 Criterion Reviews: ~1,000 tokens each √ó $0.01/1K = $0.30
- **Total**: ~$0.50-2.00 depending on models used

**Optimization**:
- Use GPT-3.5 for non-critical criteria: ~70% cost savings
- Cache extracted data: Skip re-extraction
- Batch reviews: Share context across criteria

## Hybrid Mode üé≠üöÄ

### Mix Mock and Real

You can mix modes by selectively configuring API keys:

```bash
# Only GPT-4 real, others mock
USE_MOCK_LLM=false
AZURE_OPENAI_API_KEY=your-key  # GPT-4 will be real
# ANTHROPIC_API_KEY not set      # Claude will fall back to mock
```

The system automatically falls back to mock for unavailable models.

## Choosing Models

### Available Models

| Model | Provider | Speed | Cost | Best For |
|-------|----------|-------|------|----------|
| **GPT-4** | Azure OpenAI | Medium | $$$ | Critical analysis, detailed reasoning |
| **GPT-4-Turbo** | Azure OpenAI | Fast | $$ | Balanced speed/quality |
| **GPT-3.5** | Azure OpenAI | Very Fast | $ | Quick checks, volume processing |
| **Claude-3-Sonnet** | Anthropic | Medium | $$ | Alternative perspective, nuanced |
| **Claude-3-Opus** | Anthropic | Slow | $$$$ | Highest quality, critical decisions |

### Recommended Configurations

**Budget-Conscious**:
```typescript
llmModels: ['gpt-3.5', 'gpt-3.5', 'gpt-3.5']  // All fast/cheap
```

**Balanced**:
```typescript
llmModels: ['gpt-4', 'claude-3', 'gpt-3.5']  // Default, diverse
```

**Premium Quality**:
```typescript
llmModels: ['gpt-4', 'claude-3-opus', 'gpt-4-turbo']  // Best quality
```

**Speed Optimized**:
```typescript
llmModels: ['gpt-4-turbo', 'gpt-4-turbo']  // Fast + parallel
```

## Troubleshooting

### "Model not available" Error

**Symptom**: Falls back to mock despite `USE_MOCK_LLM=false`

**Solution**:
1. Check API key is set: `echo $AZURE_OPENAI_API_KEY`
2. Verify endpoint is correct
3. Confirm deployment names match Azure
4. Test connectivity: `curl https://your-endpoint/...`

### High Costs

**Solutions**:
- Use mock mode for testing
- Switch to GPT-3.5 for non-critical criteria
- Reduce number of review agents
- Cache extraction results

### Slow Performance

**Solutions**:
- Use GPT-4-Turbo instead of GPT-4
- Reduce `maxTokens` in llm-service.ts
- Use fewer review models (2 instead of 3)
- Parallelize more aggressively

## Best Practices

### Development
- ‚úÖ Use mock mode during development
- ‚úÖ Test with real mode before production
- ‚úÖ Commit .env.template, NOT .env
- ‚úÖ Use environment variables in CI/CD

### Production
- ‚úÖ Use real mode with proper API keys
- ‚úÖ Monitor costs with Azure/Anthropic dashboards
- ‚úÖ Set up rate limiting
- ‚úÖ Cache extraction results
- ‚úÖ Log all API calls for auditing

### Security
- ‚úÖ Never commit API keys
- ‚úÖ Use Azure Key Vault for production keys
- ‚úÖ Rotate keys regularly
- ‚úÖ Use role-based access control
- ‚úÖ Monitor for unusual usage

---

**Ready to go REAL?** Set `USE_MOCK_LLM=false` and let the AI do its magic! üöÄ
